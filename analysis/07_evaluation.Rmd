---
title: "Phase 7: Comprehensive Evaluation"
author: "Fraud Detection Project"
date: "`r Sys.Date()`"
output:
  html_document:
    toc: true
    toc_float: true
    code_folding: show
    theme: flatly
    highlight: tango
    fig_width: 10
    fig_height: 6
---

```{r setup, include=FALSE}
# Determine project root directory
current_dir <- getwd()
if (basename(current_dir) == "analysis") {
  project_root <- dirname(current_dir)
  setwd(project_root)
} else {
  project_root <- current_dir
}

# Source the setup script
setup_file <- file.path(project_root, "R", "setup.R")
if (file.exists(setup_file)) {
  source(setup_file)
} else {
  stop("Cannot find setup.R at: ", setup_file)
}

# Source utility functions
source(file.path(project_root, "R", "data_utils.R"))
source(file.path(project_root, "R", "preprocessing.R"))
source(file.path(project_root, "R", "evaluation.R"))

# Set random seed for reproducibility
set.seed(42)

knitr::opts_chunk$set(
  echo = TRUE,
  warning = FALSE,
  message = FALSE,
  fig.path = "results/figures/07_evaluation-",
  cache = TRUE
)
```

## Introduction

This document provides comprehensive evaluation of the final tuned model on the **test set** (used for the first time). We'll:

- Train final model on train+validation combined data
- Evaluate on test set with comprehensive metrics
- Create confusion matrices at multiple thresholds
- Generate Precision-Recall and ROC curves
- Perform bootstrap confidence intervals
- Conduct cost-benefit analysis

## Load Data

```{r load-data}
# Load all datasets
train_raw <- readr::read_csv(file.path(paths$data, "train.csv"), show_col_types = FALSE)
val_raw <- readr::read_csv(file.path(paths$data, "validation.csv"), show_col_types = FALSE)
test_raw <- readr::read_csv(file.path(paths$data, "test.csv"), show_col_types = FALSE)

cat("=== Loading All Datasets ===\n")
cat("Training set:", nrow(train_raw), "rows\n")
cat("Validation set:", nrow(val_raw), "rows\n")
cat("Test set:", nrow(test_raw), "rows\n")

# Combine train and validation for final training
train_val_raw <- dplyr::bind_rows(train_raw, val_raw)
cat("\nCombined train+validation:", nrow(train_val_raw), "rows\n")

# Apply feature engineering
cat("\n=== Applying Feature Engineering ===\n")
train_val_raw <- create_time_features(train_val_raw)
test_raw <- create_time_features(test_raw)

train_val_raw <- create_amount_features(train_val_raw)
test_raw <- create_amount_features(test_raw)

train_val_raw <- create_interaction_features(train_val_raw)
test_raw <- create_interaction_features(test_raw)

# Load preprocessing recipe
cat("\n=== Loading Preprocessing Recipe ===\n")
recipe_fitted <- readRDS(file.path(paths$models, "preprocessing_recipe.rds"))

# Apply preprocessing
cat("Applying preprocessing recipe...\n")
train_val_preprocessed <- recipes::bake(recipe_fitted, new_data = train_val_raw)
test_preprocessed <- recipes::bake(recipe_fitted, new_data = test_raw)

# Convert Class to factor
train_val_class_char <- as.character(train_val_preprocessed$Class)
train_val_class_char[train_val_class_char == "0"] <- "Non-Fraud"
train_val_class_char[train_val_class_char == "1"] <- "Fraud"
train_val_preprocessed$Class <- factor(train_val_class_char, levels = c("Non-Fraud", "Fraud"))

test_class_char <- as.character(test_preprocessed$Class)
test_class_char[test_class_char == "0"] <- "Non-Fraud"
test_class_char[test_class_char == "1"] <- "Fraud"
test_preprocessed$Class <- factor(test_class_char, levels = c("Non-Fraud", "Fraud"))

cat("\n=== Data Ready ===\n")
cat("Train+Validation:", nrow(train_val_preprocessed), "rows\n")
cat("Test:", nrow(test_preprocessed), "rows\n")
cat("\nClass distribution (train+validation):\n")
print(table(train_val_preprocessed$Class))
cat("\nClass distribution (test):\n")
print(table(test_preprocessed$Class))
```

### Load Best Model and Hyperparameters

```{r load-best-model}
# Load best model info
best_model_file <- file.path(paths$models, "best_model_selection.rds")
best_model_info <- readRDS(best_model_file)
best_model_type <- best_model_info$model
best_technique <- best_model_info$imbalance_technique

cat("=== Best Model Configuration ===\n")
cat("Model:", best_model_type, "\n")
cat("Imbalance Technique:", best_technique, "\n")

# Load best hyperparameters if available
if (best_model_type == "Random Forest" || best_model_type == "Ensemble") {
  rf_params_file <- file.path(paths$models, "random_forest_best_params.rds")
  if (file.exists(rf_params_file)) {
    rf_best_params <- readRDS(rf_params_file)
    cat("\nRandom Forest Best Hyperparameters:\n")
    print(rf_best_params)
  }
}

if (best_model_type == "XGBoost" || best_model_type == "Ensemble") {
  xgb_params_file <- file.path(paths$models, "xgboost_best_params.rds")
  if (file.exists(xgb_params_file)) {
    xgb_best_params <- readRDS(xgb_params_file)
    cat("\nXGBoost Best Hyperparameters:\n")
    print(xgb_best_params)
  }
}

if (best_model_type == "Logistic Regression") {
  lr_params_file <- file.path(paths$models, "logistic_regression_best_params.rds")
  if (file.exists(lr_params_file)) {
    lr_best_params <- readRDS(lr_params_file)
    cat("\nLogistic Regression Best Hyperparameters:\n")
    print(lr_best_params)
  }
}
```

### Apply Imbalance Technique to Combined Training Data

```{r apply-imbalance-technique}
# Apply the imbalance technique to combined training data
cat("=== Applying", best_technique, "to Combined Training Data ===\n")

if (best_technique == "Class Weights") {
  train_val_balanced <- train_val_preprocessed
  cat("Class weights will be applied during model training\n")
  
} else if (best_technique == "SMOTE") {
  recipe_imbalance <- recipes::recipe(Class ~ ., data = train_val_preprocessed) %>%
    themis::step_smote(Class, neighbors = 5)
  recipe_imbalance <- recipes::prep(recipe_imbalance, training = train_val_preprocessed)
  train_val_balanced <- recipes::bake(recipe_imbalance, new_data = train_val_preprocessed)
  
} else if (best_technique == "Random Upsampling") {
  recipe_imbalance <- recipes::recipe(Class ~ ., data = train_val_preprocessed) %>%
    themis::step_upsample(Class)
  recipe_imbalance <- recipes::prep(recipe_imbalance, training = train_val_preprocessed)
  train_val_balanced <- recipes::bake(recipe_imbalance, new_data = train_val_preprocessed)
  
} else if (best_technique == "NearMiss Undersampling") {
  recipe_imbalance <- recipes::recipe(Class ~ ., data = train_val_preprocessed) %>%
    themis::step_nearmiss(Class, neighbors = 3)
  recipe_imbalance <- recipes::prep(recipe_imbalance, training = train_val_preprocessed)
  train_val_balanced <- recipes::bake(recipe_imbalance, new_data = train_val_preprocessed)
  
} else if (best_technique == "Random Undersampling") {
  recipe_imbalance <- recipes::recipe(Class ~ ., data = train_val_preprocessed) %>%
    themis::step_downsample(Class)
  recipe_imbalance <- recipes::prep(recipe_imbalance, training = train_val_preprocessed)
  train_val_balanced <- recipes::bake(recipe_imbalance, new_data = train_val_preprocessed)
  
} else if (best_technique == "ROSE") {
  recipe_imbalance <- recipes::recipe(Class ~ ., data = train_val_preprocessed) %>%
    themis::step_rose(Class)
  recipe_imbalance <- recipes::prep(recipe_imbalance, training = train_val_preprocessed)
  train_val_balanced <- recipes::bake(recipe_imbalance, new_data = train_val_preprocessed)
  
} else {
  train_val_balanced <- train_val_preprocessed
}

cat("\nCombined training data after balancing:\n")
cat("Rows:", nrow(train_val_balanced), "\n")
cat("Class distribution:\n")
print(table(train_val_balanced$Class))
```

## Train Final Model

```{r train-final-model}
cat("=== Training Final Model ===\n")
cat("Model:", best_model_type, "\n")
cat("Training on:", nrow(train_val_balanced), "rows\n")

if (best_model_type == "Random Forest" || best_model_type == "Ensemble") {
  # Use tuned hyperparameters if available, otherwise use defaults
  if (exists("rf_best_params")) {
    rf_spec <- parsnip::rand_forest(
      trees = rf_best_params$trees,
      min_n = rf_best_params$min_n,
      mtry = rf_best_params$mtry
    ) %>%
      parsnip::set_engine("ranger", importance = "impurity") %>%
      parsnip::set_mode("classification")
    cat("Using tuned hyperparameters\n")
  } else {
    rf_spec <- parsnip::rand_forest(
      trees = 100,
      min_n = 2,
      mtry = floor(sqrt(ncol(train_val_balanced) - 1))
    ) %>%
      parsnip::set_engine("ranger", importance = "impurity") %>%
      parsnip::set_mode("classification")
    cat("Using default hyperparameters\n")
  }
  
  rf_workflow <- workflows::workflow() %>%
    workflows::add_model(rf_spec) %>%
    workflows::add_formula(Class ~ .)
  
  final_model <- parsnip::fit(rf_workflow, data = train_val_balanced)
  
} else if (best_model_type == "XGBoost" || best_model_type == "Ensemble") {
  # Use tuned hyperparameters if available
  if (exists("xgb_best_params")) {
    xgb_spec <- parsnip::boost_tree(
      trees = xgb_best_params$trees,
      tree_depth = xgb_best_params$tree_depth,
      learn_rate = xgb_best_params$learn_rate,
      min_n = xgb_best_params$min_n
    ) %>%
      parsnip::set_engine("xgboost") %>%
      parsnip::set_mode("classification")
    cat("Using tuned hyperparameters\n")
  } else {
    xgb_spec <- parsnip::boost_tree(
      trees = 100,
      tree_depth = 6,
      learn_rate = 0.1,
      min_n = 2
    ) %>%
      parsnip::set_engine("xgboost") %>%
      parsnip::set_mode("classification")
    cat("Using default hyperparameters\n")
  }
  
  xgb_workflow <- workflows::workflow() %>%
    workflows::add_model(xgb_spec) %>%
    workflows::add_formula(Class ~ .)
  
  final_model <- parsnip::fit(xgb_workflow, data = train_val_balanced)
  
} else if (best_model_type == "Logistic Regression") {
  # Use tuned hyperparameters if available
  if (exists("lr_best_params")) {
    lr_spec <- parsnip::logistic_reg(
      penalty = lr_best_params$penalty,
      mixture = lr_best_params$mixture
    ) %>%
      parsnip::set_engine("glmnet") %>%
      parsnip::set_mode("classification")
    cat("Using tuned hyperparameters\n")
  } else {
    lr_spec <- parsnip::logistic_reg() %>%
      parsnip::set_engine("glm") %>%
      parsnip::set_mode("classification")
    cat("Using default hyperparameters\n")
  }
  
  lr_workflow <- workflows::workflow() %>%
    workflows::add_model(lr_spec) %>%
    workflows::add_formula(Class ~ .)
  
  final_model <- parsnip::fit(lr_workflow, data = train_val_balanced)
}

cat("\nFinal model trained successfully.\n")
```

## Test Set Evaluation

```{r test-evaluation}
cat("=== Test Set Evaluation ===\n")
cat("Evaluating on", nrow(test_preprocessed), "test samples\n")

# Get predictions
test_pred <- predict(final_model, new_data = test_preprocessed, type = "class")
test_prob <- predict(final_model, new_data = test_preprocessed, type = "prob")

test_truth <- test_preprocessed$Class

# Calculate comprehensive metrics
test_metrics <- calculate_all_metrics(
  truth = test_truth,
  estimate = test_pred$.pred_class,
  prob = test_prob$.pred_Fraud
)

cat("\n=== Test Set Metrics ===\n")
print(test_metrics)

# Create confusion matrix at default threshold (0.5)
cm_default <- create_confusion_matrix(test_truth, test_pred$.pred_class)

cat("\n=== Confusion Matrix (Threshold = 0.5) ===\n")
print(cm_default)
```

## Confusion Matrices at Multiple Thresholds

```{r multiple-thresholds}
cat("=== Confusion Matrices at Multiple Thresholds ===\n")

thresholds <- c(0.3, 0.4, 0.5, 0.6, 0.7)
threshold_results <- list()

for (thresh in thresholds) {
  # Convert probabilities to predictions at this threshold
  pred_at_thresh <- factor(
    ifelse(test_prob$.pred_Fraud > thresh, "Fraud", "Non-Fraud"),
    levels = c("Non-Fraud", "Fraud")
  )
  
  # Calculate metrics
  metrics_at_thresh <- calculate_all_metrics(
    truth = test_truth,
    estimate = pred_at_thresh,
    prob = test_prob$.pred_Fraud
  )
  
  # Create confusion matrix
  cm_at_thresh <- create_confusion_matrix(test_truth, pred_at_thresh)
  
  threshold_results[[as.character(thresh)]] <- list(
    threshold = thresh,
    metrics = metrics_at_thresh,
    confusion_matrix = cm_at_thresh
  )
  
  cat("\n--- Threshold =", thresh, "---\n")
  cat("Precision:", round(metrics_at_thresh$Precision, 4), "\n")
  cat("Recall:", round(metrics_at_thresh$Recall, 4), "\n")
  cat("F1:", round(metrics_at_thresh$F1, 4), "\n")
}

# Create comparison table
threshold_comparison <- dplyr::bind_rows(
  lapply(threshold_results, function(x) {
    x$metrics %>%
      dplyr::mutate(Threshold = x$threshold) %>%
      dplyr::select(Threshold, Precision, Recall, F1, MCC)
  })
)

cat("\n=== Threshold Comparison ===\n")
print(threshold_comparison)
```

## Precision-Recall Curve

```{r pr-curve}
cat("=== Precision-Recall Curve ===\n")

# Create data frame for PR curve
pr_data <- data.frame(
  truth = test_truth,
  estimate = test_prob$.pred_Fraud
)

# Create PR curve data
pr_curve <- yardstick::pr_curve(pr_data, truth, estimate)

# Calculate PR-AUC
pr_auc <- yardstick::pr_auc(pr_data, truth, estimate)

p_pr <- pr_curve %>%
  ggplot2::ggplot(aes(x = recall, y = precision)) +
  ggplot2::geom_path(linewidth = 1, color = "steelblue") +
  ggplot2::geom_abline(intercept = sum(test_truth == "Fraud") / length(test_truth), 
                       slope = 0, linetype = "dashed", color = "red") +
  ggplot2::labs(
    title = "Precision-Recall Curve",
    subtitle = paste("PR-AUC =", round(pr_auc$.estimate, 4)),
    x = "Recall",
    y = "Precision"
  ) +
  ggplot2::xlim(0, 1) +
  ggplot2::ylim(0, 1) +
  ggplot2::theme_minimal()

print(p_pr)

cat("PR-AUC:", round(pr_auc$.estimate, 4), "\n")
```

## ROC Curve

```{r roc-curve}
cat("=== ROC Curve ===\n")

# Create data frame for ROC curve
roc_data <- data.frame(
  truth = test_truth,
  estimate = test_prob$.pred_Fraud
)

# Create ROC curve data
roc_curve <- yardstick::roc_curve(roc_data, truth, estimate)

# Calculate ROC-AUC
roc_auc <- yardstick::roc_auc(roc_data, truth, estimate)

p_roc <- roc_curve %>%
  ggplot2::ggplot(aes(x = 1 - specificity, y = sensitivity)) +
  ggplot2::geom_path(linewidth = 1, color = "steelblue") +
  ggplot2::geom_abline(intercept = 0, slope = 1, linetype = "dashed", color = "red") +
  ggplot2::labs(
    title = "ROC Curve",
    subtitle = paste("ROC-AUC =", round(roc_auc$.estimate, 4)),
    x = "1 - Specificity (False Positive Rate)",
    y = "Sensitivity (True Positive Rate)"
  ) +
  ggplot2::xlim(0, 1) +
  ggplot2::ylim(0, 1) +
  ggplot2::theme_minimal()

print(p_roc)

cat("ROC-AUC:", round(roc_auc$.estimate, 4), "\n")
```

## Bootstrap Confidence Intervals

```{r bootstrap-ci}
cat("=== Bootstrap Confidence Intervals ===\n")

# Bootstrap function for PR-AUC
bootstrap_pr_auc <- function(truth, prob, n_bootstrap = 1000) {
  n <- length(truth)
  pr_aucs <- numeric(n_bootstrap)
  
  for (i in 1:n_bootstrap) {
    # Sample with replacement
    idx <- sample(1:n, n, replace = TRUE)
    truth_boot <- truth[idx]
    prob_boot <- prob[idx]
    
    # Calculate PR-AUC
    pr_aucs[i] <- yardstick::pr_auc_vec(truth_boot, prob_boot)
  }
  
  return(pr_aucs)
}

# Calculate bootstrap CIs
cat("Performing bootstrap (1000 iterations)...\n")
pr_auc_boot <- bootstrap_pr_auc(test_truth, test_prob$.pred_Fraud, n_bootstrap = 1000)

# Calculate confidence intervals
ci_95 <- quantile(pr_auc_boot, c(0.025, 0.975))
ci_90 <- quantile(pr_auc_boot, c(0.05, 0.95))

cat("\nPR-AUC Bootstrap Confidence Intervals:\n")
cat("  95% CI: [", round(ci_95[1], 4), ", ", round(ci_95[2], 4), "]\n", sep = "")
cat("  90% CI: [", round(ci_90[1], 4), ", ", round(ci_90[2], 4), "]\n", sep = "")
cat("  Mean:", round(mean(pr_auc_boot), 4), "\n")
cat("  SD:", round(sd(pr_auc_boot), 4), "\n")

# Plot bootstrap distribution
p_boot <- data.frame(PR_AUC = pr_auc_boot) %>%
  ggplot2::ggplot(aes(x = PR_AUC)) +
  ggplot2::geom_histogram(bins = 50, fill = "steelblue", alpha = 0.7) +
  ggplot2::geom_vline(xintercept = pr_auc$.estimate, color = "red", linetype = "dashed", linewidth = 1) +
  ggplot2::geom_vline(xintercept = ci_95, color = "darkred", linetype = "dotted", linewidth = 0.8) +
  ggplot2::labs(
    title = "Bootstrap Distribution of PR-AUC",
    subtitle = "Red line: observed PR-AUC, Dotted lines: 95% CI",
    x = "PR-AUC",
    y = "Frequency"
  ) +
  ggplot2::theme_minimal()

print(p_boot)
```

## Cost-Benefit Analysis

```{r cost-analysis}
cat("=== Cost-Benefit Analysis ===\n")

# Load cost matrix
cost_matrix <- list(
  TP = 0,
  FP = 10,
  FN = 100,
  TN = 0
)

cat("Cost Matrix:\n")
cat("  TP (True Positive): $", cost_matrix$TP, "\n")
cat("  FP (False Positive): $", cost_matrix$FP, "\n")
cat("  FN (False Negative): $", cost_matrix$FN, "\n")
cat("  TN (True Negative): $", cost_matrix$TN, "\n\n")

# Calculate costs at different thresholds
costs_by_threshold <- lapply(thresholds, function(thresh) {
  pred_at_thresh <- factor(
    ifelse(test_prob$.pred_Fraud > thresh, "Fraud", "Non-Fraud"),
    levels = c("Non-Fraud", "Fraud")
  )
  
  costs <- calculate_cost_metrics(test_truth, pred_at_thresh, cost_matrix)
  n_test <- length(test_truth)
  
  return(data.frame(
    Threshold = thresh,
    Total_Cost = costs$total_cost,
    Cost_per_Transaction = costs$total_cost / n_test
  ))
})

costs_df <- dplyr::bind_rows(costs_by_threshold)

cat("=== Cost Analysis by Threshold ===\n")
print(costs_df)

# Find optimal threshold (minimum cost)
optimal_threshold_idx <- which.min(costs_df$Total_Cost)
optimal_threshold <- costs_df$Threshold[optimal_threshold_idx]
optimal_cost <- costs_df$Total_Cost[optimal_threshold_idx]

cat("\nOptimal Threshold (Minimum Cost):", optimal_threshold, "\n")
cat("Minimum Total Cost: $", optimal_cost, "\n")
cat("Cost per Transaction: $", round(optimal_cost / length(test_truth), 2), "\n")

# Plot cost by threshold
p_cost <- costs_df %>%
  ggplot2::ggplot(aes(x = Threshold, y = Total_Cost)) +
  ggplot2::geom_line(linewidth = 1, color = "steelblue") +
  ggplot2::geom_point(size = 3, color = "steelblue") +
  ggplot2::geom_vline(xintercept = optimal_threshold, linetype = "dashed", color = "red") +
  ggplot2::labs(
    title = "Total Cost by Classification Threshold",
    subtitle = paste("Optimal threshold:", optimal_threshold),
    x = "Threshold",
    y = "Total Cost ($)"
  ) +
  ggplot2::theme_minimal()

print(p_cost)
```

## Summary

```{r summary}
cat("=== Comprehensive Evaluation Summary ===\n\n")

cat("1. Test Set Performance:\n")
cat("   PR-AUC:", round(test_metrics$PR_AUC, 4), "\n")
cat("   ROC-AUC:", round(test_metrics$ROC_AUC, 4), "\n")
cat("   Precision:", round(test_metrics$Precision, 4), "\n")
cat("   Recall:", round(test_metrics$Recall, 4), "\n")
cat("   F1 Score:", round(test_metrics$F1, 4), "\n")
cat("   MCC:", round(test_metrics$MCC, 4), "\n\n")

cat("2. Bootstrap Confidence Intervals (PR-AUC):\n")
cat("   95% CI: [", round(ci_95[1], 4), ", ", round(ci_95[2], 4), "]\n\n", sep = "")

cat("3. Optimal Threshold (Cost-Based):\n")
cat("   Threshold:", optimal_threshold, "\n")
cat("   Total Cost: $", optimal_cost, "\n")
cat("   Cost per Transaction: $", round(optimal_cost / length(test_truth), 2), "\n\n")

cat("4. Model Configuration:\n")
cat("   Model:", best_model_type, "\n")
cat("   Imbalance Technique:", best_technique, "\n")
cat("   Training Samples:", nrow(train_val_balanced), "\n")
cat("   Test Samples:", nrow(test_preprocessed), "\n")
```

## Save Results

```{r save-results}
# Save test metrics
readr::write_csv(test_metrics, file.path(paths$tables, "test_set_metrics.csv"))
readr::write_csv(threshold_comparison, file.path(paths$tables, "threshold_comparison.csv"))
readr::write_csv(costs_df, file.path(paths$tables, "cost_analysis.csv"))

# Save final model
saveRDS(final_model, file.path(paths$models, "final_model.rds"))

# Save evaluation results
evaluation_results <- list(
  test_metrics = test_metrics,
  pr_auc = pr_auc$.estimate,
  roc_auc = roc_auc$.estimate,
  bootstrap_ci_95 = ci_95,
  optimal_threshold = optimal_threshold,
  optimal_cost = optimal_cost,
  threshold_comparison = threshold_comparison
)
saveRDS(evaluation_results, file.path(paths$models, "evaluation_results.rds"))

cat("=== Results Saved ===\n")
cat("Test metrics:", file.path(paths$tables, "test_set_metrics.csv"), "\n")
cat("Threshold comparison:", file.path(paths$tables, "threshold_comparison.csv"), "\n")
cat("Cost analysis:", file.path(paths$tables, "cost_analysis.csv"), "\n")
cat("Final model:", file.path(paths$models, "final_model.rds"), "\n")
```

## Summary

- ✓ Trained final model on train+validation combined data
- ✓ Evaluated on test set (first time test set is used)
- ✓ Reported comprehensive metrics (Precision, Recall, F1, MCC, PR-AUC, ROC-AUC)
- ✓ Created confusion matrices at multiple thresholds (0.3, 0.4, 0.5, 0.6, 0.7)
- ✓ Generated Precision-Recall and ROC curves
- ✓ Calculated bootstrap confidence intervals for PR-AUC
- ✓ Performed cost-benefit analysis and identified optimal threshold
- ✓ Saved all results and final model

**Key Findings**: 
- Test Set PR-AUC: `r round(test_metrics$PR_AUC, 4)`
- Optimal Threshold: `r optimal_threshold` (cost-based)
- Model: `r best_model_type` with `r best_technique`

**Next Step**: Proceed to `08_threshold_optimization.Rmd` for detailed threshold analysis and optimization.
